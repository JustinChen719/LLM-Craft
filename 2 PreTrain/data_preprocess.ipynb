{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 读取数据",
   "id": "5810723547c7217c"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-23T07:22:50.973718Z",
     "start_time": "2025-03-23T07:22:49.234613Z"
    }
   },
   "source": [
    "import os\n",
    "from datasets import load_dataset\n",
    "\n",
    "os.environ['HF_DATASETS_CACHE'] = \"/root/lanyun-tmp/hf/cache\"\n",
    "\n",
    "dataset = load_dataset(\n",
    "    path=\"/root/lanyun-tmp/hf/wikipedia-zh-mnbvc\",\n",
    "    cache_dir=\"/root/lanyun-tmp/hf/cache\",\n",
    "    num_proc=36,\n",
    ")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 处理数据",
   "id": "8a8e9428df0cbf36"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def process_segment(sub_dataset, output_dir, shard_idx=0):\n",
    "    \"\"\"处理数据分片并保存为预训练格式\n",
    "    Args:\n",
    "        sub_dataset: 数据集分片（可迭代对象）\n",
    "        output_dir: 输出目录路径\n",
    "        shard_idx: 分片索引（用于多进程文件命名）\n",
    "    \"\"\"\n",
    "    # 配置参数\n",
    "    eos_token = \"<|endoftext|>\"\n",
    "    max_seq_length = 512  # 最大序列长度（含结束符）\n",
    "    chunk_size = max_seq_length - len(eos_token)  # 实际文本块长度\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    output_path = os.path.join(output_dir, f\"pretrain_{shard_idx:04d}.txt\")\n",
    "    progress_bar = tqdm(\n",
    "        desc=f\"Processing Shard {shard_idx}\",\n",
    "        unit=\"page\",\n",
    "        mininterval=1  # 降低更新频率提升性能\n",
    "    )\n",
    "\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as writer:\n",
    "        for page in sub_dataset:\n",
    "            # 1. 合并段落内容\n",
    "            try:\n",
    "                text = \"\".join(\n",
    "                    str(p[\"内容\"]) for p in page.get(\"段落\", [])  # 防御性数据访问\n",
    "                    if \"内容\" in p  # 过滤无效段落\n",
    "                )\n",
    "            except KeyError as e:\n",
    "                print(f\"跳过无效段落：{str(e)}\")\n",
    "                continue\n",
    "            # 分块处理\n",
    "            for start_idx in range(0, len(text), chunk_size):\n",
    "                # 计算分块区间\n",
    "                end_idx = start_idx + chunk_size\n",
    "                chunk = text[start_idx:end_idx]\n",
    "                processed_chunk = chunk + eos_token\n",
    "                if len(processed_chunk) > max_seq_length:\n",
    "                    processed_chunk = processed_chunk[:max_seq_length]  # 硬截断\n",
    "                writer.write(processed_chunk + \"\\n\")  # 换行分隔不同样本\n",
    "            progress_bar.update(1)\n",
    "    progress_bar.close()\n",
    "\n",
    "\n",
    "# 多进程调用示例（需要修改parallel_chunking函数）\n",
    "def parallel_chunking(dataset, output_dir, workers=32):\n",
    "    \"\"\"并行分片处理\"\"\"\n",
    "    from concurrent.futures import ProcessPoolExecutor\n",
    "\n",
    "    with ProcessPoolExecutor(max_workers=workers) as executor:\n",
    "        futures = []\n",
    "        for i in range(workers):\n",
    "            # 获取数据分片（需根据实际数据集结构修改）\n",
    "            sub_data = dataset[\"train\"].shard(num_shards=workers, index=i)\n",
    "\n",
    "            # 提交任务时传递分片索引\n",
    "            futures.append(\n",
    "                executor.submit(\n",
    "                    process_segment,\n",
    "                    sub_dataset=sub_data,\n",
    "                    output_dir=output_dir,\n",
    "                    shard_idx=i\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # 等待所有任务完成\n",
    "        for f in futures:\n",
    "            f.result()\n",
    "\n",
    "\n",
    "processed_data = parallel_chunking(dataset, output_dir=\"./data/\")"
   ],
   "id": "eb64d14def777c57",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
